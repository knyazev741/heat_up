"""
SearchAgent V2 - —Å —Ä–µ–∞–ª—å–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º –Ω–∞ —Å–∞–π—Ç—ã –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—Å—ã–ª–æ–∫
"""

import re
import logging
import asyncio
from typing import Dict, Any, List, Set
from duckduckgo_search import DDGS
from openai import OpenAI
from config import settings
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)


class SearchAgentV2:
    """
    –ê–≥–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º web scraping
    
    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–≥–æ—Ä–æ–¥ + –∏–Ω—Ç–µ—Ä–µ—Å—ã)
    2. –ò—â–µ—Ç –≤ DuckDuckGo
    3. –ü–ï–†–ï–•–û–î–ò–¢ –ù–ê –°–ê–ô–¢–´ –∏ —Å–∫—Ä–µ–π–ø–∏—Ç HTML
    4. –ò–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-—Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    5. LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤
    """
    
    def __init__(self):
        # Using DeepSeek API (OpenAI-compatible)
        self.client = OpenAI(
            api_key=settings.deepseek_api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "deepseek-chat"
        
        # HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        self.http_client = httpx.AsyncClient(
            timeout=10.0,  # 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–ø—Ä–æ—Å
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
        )
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ (–Ω–µ —Å–∫—Ä–µ–π–ø–∏–º)
        self.blacklisted_domains = {
            'instagram.com', 'facebook.com', 'twitter.com', 'x.com',
            'youtube.com', 'tiktok.com', 'linkedin.com', 'reddit.com',
            'pinterest.com', 'amazon.com', 'ebay.com'
        }
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ username
        self.blacklisted_usernames = {
            'gmail', 'mail', 'yandex', 'yahoo', 'outlook', 'hotmail', 'icloud',
            'protonmail', 'aol', 'zoho', 'mailru', 'rambler',
            'instagram', 'facebook', 'twitter', 'tiktok', 'youtube', 'linkedin',
            'whatsapp', 'viber', 'skype', 'discord', 'snapchat',
            'pinterest', 'reddit', 'tumblr', 'flickr', 'vimeo',
            'amazon', 'ebay', 'aliexpress', 'spotify', 'netflix',
            'google', 'microsoft', 'apple', 'samsung', 'huawei',
            'twitch', 'steam', 'playstation', 'xbox', 'nintendo',
            'badoo', 'tinder', 'bumble', 'hinge', 'okcupid',
            'magenta', 'telekom', 'vodafone', 'orange', 't-mobile',
            'katyperry', 'justinbieber', 'arianagrande', 'selenagomez'
        }
    
    async def find_relevant_chats(
        self,
        persona: Dict[str, Any],
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        –ò—â–µ—Ç –†–ï–ê–õ–¨–ù–´–ï Telegram-—á–∞—Ç—ã —á–µ—Ä–µ–∑ DuckDuckGo + web scraping
        
        Args:
            persona: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω—ã
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info(f"üîç Searching REAL Telegram chats for: {persona.get('generated_name')}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        queries = self._generate_search_queries(persona)
        logger.info(f"Generated {len(queries)} search queries")
        
        # –ò—â–µ–º –≤ DuckDuckGo (–ø–æ–ª—É—á–∞–µ–º URLs)
        urls_to_scrape = await self._search_duckduckgo(queries)
        logger.info(f"Got {len(urls_to_scrape)} URLs to scrape")
        
        # –°–∫—Ä–µ–π–ø–∏–º —Å–∞–π—Ç—ã
        all_channels = await self._scrape_websites(urls_to_scrape)
        logger.info(f"Found {len(all_channels)} UNIQUE channels after scraping")
        
        if not all_channels:
            logger.warning("No channels found via web scraping!")
            return []
        
        # LLM —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_chats = await self._rank_chats_with_llm(persona, list(all_channels.values()))
        
        return ranked_chats[:limit]
    
    def _generate_search_queries(self, persona: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        
        city = persona.get('city', '–ú–æ—Å–∫–≤–∞')
        interests = persona.get('interests', [])
        occupation = persona.get('occupation', '')
        
        queries = []
        
        # –ì–†–£–ü–ü–´ –ì–û–†–û–î–ê (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        queries.extend([
            f"{city} telegram group chat",
            f"t.me {city} –∂–∏—Ç–µ–ª–∏",
            f"telegram —á–∞—Ç {city} –æ–±—â–µ–Ω–∏–µ",
            f"{city} telegram –≥—Ä—É–ø–ø–∞",
        ])
        
        # –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ì–†–£–ü–ü–´ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º
        for interest in interests[:3]:  # –¢–æ–ø-3 –∏–Ω—Ç–µ—Ä–µ—Å–∞
            queries.extend([
                f"{city} {interest} telegram",
                f"t.me {interest} {city}",
                f"telegram –≥—Ä—É–ø–ø–∞ {interest}",
            ])
        
        # –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ï –≥—Ä—É–ø–ø—ã
        if occupation:
            queries.append(f"telegram {occupation} {city}")
        
        # –ù–û–í–û–°–¢–ò –ì–û–†–û–î–ê
        queries.append(f"telegram –∫–∞–Ω–∞–ª {city} –Ω–æ–≤–æ—Å—Ç–∏")
        
        return queries[:15]  # –ú–∞–∫—Å 15 –∑–∞–ø—Ä–æ—Å–æ–≤
    
    async def _search_duckduckgo(self, queries: List[str]) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –≤ DuckDuckGo –∏ —Å–æ–±–∏—Ä–∞–µ—Ç URLs –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞"""
        
        urls_to_scrape = []
        seen_urls = set()
        
        with DDGS() as ddgs:
            for q in queries:
                logger.info(f"Searching DuckDuckGo: {q}")
                try:
                    results = list(ddgs.text(q, max_results=10))
                    logger.info(f"  ‚Üí Got {len(results)} results")
                    
                    for r in results:
                        url = r.get('href', '')
                        if url and url not in seen_urls:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
                            domain = urlparse(url).netloc.lower()
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                            if any(bd in domain for bd in self.blacklisted_domains):
                                logger.debug(f"  ‚úó Skipping blacklisted domain: {domain}")
                                continue
                            
                            urls_to_scrape.append({
                                'url': url,
                                'title': r.get('title', ''),
                                'snippet': r.get('body', ''),
                                'query': q
                            })
                            seen_urls.add(url)
                    
                except Exception as e:
                    logger.error(f"Search error for '{q}': {e}")
                    continue
        
        return urls_to_scrape
    
    async def _scrape_websites(self, urls: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        –°–∫—Ä–µ–π–ø–∏—Ç —Å–∞–π—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã
        
        Returns:
            {username: {channel_data}}
        """
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
        urls_to_process = urls[:30]  # –ú–∞–∫—Å 30 —Å–∞–π—Ç–æ–≤
        
        logger.info(f"üåê Scraping {len(urls_to_process)} websites...")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥
        tasks = [self._scrape_single_page(url_data) for url_data in urls_to_process]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
        all_channels = {}
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Error scraping {urls_to_process[i]['url']}: {result}")
                continue
            
            if result:
                for username, channel_data in result.items():
                    if username not in all_channels:
                        all_channels[username] = channel_data
        
        return all_channels
    
    async def _scrape_single_page(self, url_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """
        –°–∫—Ä–µ–π–ø–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã
        
        Returns:
            {username: {channel_data}}
        """
        url = url_data['url']
        logger.info(f"  üìÑ Scraping: {url[:80]}...")
        
        try:
            # HTTP –∑–∞–ø—Ä–æ—Å
            response = await self.http_client.get(url)
            response.raise_for_status()
            
            html = response.text
            logger.info(f"    ‚úì Loaded {len(html)} chars")
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'lxml')
            
            # –£–¥–∞–ª—è–µ–º script –∏ style —Ç–µ–≥–∏ (–Ω–µ –Ω—É–∂–Ω—ã)
            for tag in soup(['script', 'style']):
                tag.decompose()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–Ω–∞–ª—ã
            channels = self._extract_channels_from_html(soup, page_text, url_data)
            
            logger.info(f"    ‚úì Extracted {len(channels)} channels")
            
            return channels
            
        except httpx.TimeoutException:
            logger.warning(f"    ‚úó Timeout: {url[:60]}")
            return {}
        except httpx.HTTPStatusError as e:
            logger.warning(f"    ‚úó HTTP {e.response.status_code}: {url[:60]}")
            return {}
        except Exception as e:
            logger.error(f"    ‚úó Error: {e}")
            return {}
    
    def _extract_channels_from_html(
        self,
        soup: BeautifulSoup,
        page_text: str,
        url_data: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã –∏–∑ HTML
        
        Returns:
            {username: {channel_data}}
        """
        
        channels = {}
        
        # 1. –ü–†–ò–û–†–ò–¢–ï–¢: –ò—â–µ–º –≤—Å–µ t.me/ —Å—Å—ã–ª–∫–∏ –≤ <a> —Ç–µ–≥–∞—Ö
        telegram_links = soup.find_all('a', href=re.compile(r'(t\.me|telegram\.me)/'))
        
        for link in telegram_links:
            href = link.get('href', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∏–∑ —Å—Å—ã–ª–∫–∏
            match = re.search(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', href)
            if match:
                username = match.group(1)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º
                if not self._is_valid_username(username):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–∫–∏
                link_text = link.get_text(strip=True)
                parent_text = link.parent.get_text(strip=True) if link.parent else ''
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': link_text[:100] or username,
                        'description': parent_text[:200],
                        'source_url': url_data['url'],
                        'confidence': 'high'  # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –≤ <a> —Ç–µ–≥–µ
                    }
        
        # 2. –ò—â–µ–º t.me/ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ (–º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –≤ <a>)
        text_links = re.findall(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', page_text)
        
        for username in text_links:
            if not self._is_valid_username(username):
                continue
            
            if username not in channels:
                channels[username] = {
                    'username': f"@{username}",
                    'title': username,
                    'description': url_data.get('snippet', '')[:200],
                    'source_url': url_data['url'],
                    'confidence': 'medium'  # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç–µ
                }
        
        # 3. –ò—â–µ–º @username —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Telegram)
        page_text_lower = page_text.lower()
        has_telegram_context = any(
            keyword in page_text_lower 
            for keyword in ['telegram', 't.me', '—Ç–µ–ª–µ–≥—Ä–∞–º', '—Ç–µ–ª–µ–≥—Ä–∞–º–º', '—Ç–µ–ª–µ–≥–∞']
        )
        
        if has_telegram_context:
            mentions = re.findall(r'@([a-zA-Z][a-zA-Z0-9_]{4,31})', page_text)
            
            for username in set(mentions):  # set() –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                if not self._is_valid_username(username):
                    continue
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': username,
                        'description': url_data.get('snippet', '')[:200],
                        'source_url': url_data['url'],
                        'confidence': 'low'  # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –±–µ–∑ t.me/
                    }
        
        return channels
    
    def _is_valid_username(self, username: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å Telegram username"""
        
        username_lower = username.lower()
        
        # 1. –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫
        if username_lower in self.blacklisted_usernames:
            return False
        
        # 2. –î–ª–∏–Ω–∞
        if len(username) < 5 or len(username) > 32:
            return False
        
        # 3. –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫—É (email-–ø–æ–¥–æ–±–Ω—ã–µ)
        if '.' in username:
            return False
        
        # 4. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è (Instagram –ø–∞—Ç—Ç–µ—Ä–Ω)
        if '___' in username or username.endswith('_') or username.startswith('_'):
            return False
        
        # 5. –î–ª–∏–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –≤ –∫–æ–Ω—Ü–µ (user12345)
        if re.search(r'\d{3,}$', username):
            return False
        
        # 6. –î–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –±—É–∫–≤—ã
        if not username[0].isalpha():
            return False
        
        return True
    
    async def _rank_chats_with_llm(
        self,
        persona: Dict[str, Any],
        channels: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
        
        if not channels:
            return []
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è LLM (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ = –¥–æ—Ä–æ–≥–æ)
        channels_for_llm = channels[:30]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM
        channels_list = "\n".join([
            f"{i+1}. {ch['username']} - {ch['title']} ({ch.get('description', '')[:80]})"
            for i, ch in enumerate(channels_for_llm)
        ])
        
        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Telegram. –û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤/–≥—Ä—É–ø–ø –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
- –ì–æ—Ä–æ–¥: {persona.get('city')}
- –ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(persona.get('interests', []))}
- –ü—Ä–æ—Ñ–µ—Å—Å–∏—è: {persona.get('occupation')}
- –í–æ–∑—Ä–∞—Å—Ç: {persona.get('age')}

–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã:
{channels_list}

–û—Ü–µ–Ω–∏ –ö–ê–ñ–î–´–ô –∫–∞–Ω–∞–ª –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0.0-1.0):
- 1.0 = –ò–î–ï–ê–õ–¨–ù–û –ø–æ–¥—Ö–æ–¥–∏—Ç (–≥—Ä—É–ø–ø–∞ –≥–æ—Ä–æ–¥–∞, —Ç–æ—á–Ω–æ–µ –ø–æ–ø–∞–¥–∞–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º)
- 0.8 = –û—Ç–ª–∏—á–Ω–æ (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä—É–ø–ø–∞ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º)
- 0.5 = –°—Ä–µ–¥–Ω–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ)
- 0.3 = –°–ª–∞–±–æ (–∫–æ—Å–≤–µ–Ω–Ω–∞—è —Å–≤—è–∑—å)
- 0.0 = –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç

–ü–†–ò–û–†–ò–¢–ï–¢–´:
1. –ì—Ä—É–ø–ø—ã/—á–∞—Ç—ã –ì–û–†–û–î–ê –¥–ª—è –æ–±—â–µ–Ω–∏—è (group/supergroup) - –í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢!
2. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã –ø–æ –ò–ù–¢–ï–†–ï–°–ê–ú –≤ –≥–æ—Ä–æ–¥–µ
3. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
4. –û–±—â–∏–µ –∫–∞–Ω–∞–ª—ã –≥–æ—Ä–æ–¥–∞ (–Ω–æ–≤–æ—Å—Ç–∏)

–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø (group/channel/supergroup) - –≥—Ä—É–ø–ø—ã –ª—É—á—à–µ —á–µ–º –∫–∞–Ω–∞–ª—ã!

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - JSON –º–∞—Å—Å–∏–≤:
[
  {{
    "username": "@example",
    "relevance_score": 0.9,
    "chat_type": "group",
    "reason": "–ì—Ä—É–ø–ø–∞ –≥–æ—Ä–æ–¥–∞ –¥–ª—è –æ–±—â–µ–Ω–∏—è"
  }},
  ...
]

–¢–û–õ–¨–ö–û JSON!"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                temperature=0.3,
                max_tokens=3000,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Telegram. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            response_text = response.choices[0].message.content
            
            # Parse JSON
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0].strip()
            else:
                json_str = response_text.strip()
            
            import json
            rankings = json.loads(json_str)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            username_to_channel = {ch['username']: ch for ch in channels_for_llm}
            
            ranked_chats = []
            for rank in rankings:
                username = rank.get('username', '')
                if username in username_to_channel:
                    original = username_to_channel[username]
                    ranked_chats.append({
                        "chat_username": username,
                        "chat_title": original.get('title', ''),
                        "chat_description": original.get('description', ''),
                        "chat_type": rank.get('chat_type', 'unknown'),
                        "member_count": None,
                        "relevance_score": rank.get('relevance_score', 0.5),
                        "relevance_reason": rank.get('reason', '')
                    })
            
            logger.info(f"LLM ranked {len(ranked_chats)} chats")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-5
            for i, chat in enumerate(sorted(ranked_chats, key=lambda x: x['relevance_score'], reverse=True)[:5]):
                logger.info(f"  {i+1}. {chat['chat_username']} ({chat['chat_type']}, score: {chat['relevance_score']:.2f})")
            
            return ranked_chats
            
        except Exception as e:
            logger.error(f"Error ranking chats: {e}")
            
            # Fallback - –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–Ω–∞–ª—ã –∫–∞–∫ –µ—Å—Ç—å
            return [{
                "chat_username": ch['username'],
                "chat_title": ch.get('title', ''),
                "chat_description": ch.get('description', ''),
                "chat_type": "unknown",
                "member_count": None,
                "relevance_score": 0.5,
                "relevance_reason": "Found via search"
            } for ch in channels_for_llm]
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        await self.http_client.aclose()




