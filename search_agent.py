"""
SearchAgent - REAL web search –¥–ª—è Telegram –∫–∞–Ω–∞–ª–æ–≤ —á–µ—Ä–µ–∑ Google Custom Search API
"""

import re
import logging
import asyncio
from typing import Dict, Any, List
from urllib.parse import urlparse
import httpx
from bs4 import BeautifulSoup
from openai import OpenAI
from config import settings
import json

logger = logging.getLogger(__name__)


class SearchAgent:
    """
    –ê–≥–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –†–ï–ê–õ–¨–ù–´–• Telegram-—á–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Google Custom Search API
    
    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–≥–æ—Ä–æ–¥ + –∏–Ω—Ç–µ—Ä–µ—Å—ã)
    2. –ò—â–µ—Ç –≤ Google Custom Search API
    3. –ü–ï–†–ï–•–û–î–ò–¢ –ù–ê –°–ê–ô–¢–´ –∏ —Å–∫—Ä–µ–π–ø–∏—Ç HTML
    4. –ò–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-—Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    5. LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤
    """
    
    def __init__(self):
        # Using DeepSeek API (OpenAI-compatible)
        self.client = OpenAI(
            api_key=settings.deepseek_api_key,
            base_url="https://api.deepseek.com"
        )
        self.model = "deepseek-chat"
        
        # Google Custom Search API
        self.google_api_key = settings.google_search_api_key
        self.google_engine_id = settings.google_search_engine_id
        self.google_api_url = "https://www.googleapis.com/customsearch/v1"
        
        if not self.google_api_key or not self.google_engine_id:
            logger.warning("Google Search API not configured - search will be limited")
        
        # HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è web scraping
        self.http_client = httpx.AsyncClient(
            timeout=15.0,
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
        )
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ (–Ω–µ —Å–∫—Ä–µ–π–ø–∏–º)
        self.blacklisted_domains = {
            'instagram.com', 'facebook.com', 'twitter.com', 'x.com',
            'youtube.com', 'tiktok.com', 'linkedin.com', 'reddit.com',
            'pinterest.com', 'amazon.com', 'ebay.com'
        }
    
    async def find_relevant_chats(
        self,
        persona: Dict[str, Any],
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        –ò—â–µ—Ç –†–ï–ê–õ–¨–ù–´–ï Telegram-—á–∞—Ç—ã —á–µ—Ä–µ–∑ Google Custom Search API + web scraping
        
        Args:
            persona: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω—ã
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info(f"üîç Searching REAL Telegram chats for: {persona.get('generated_name')}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Google API
        if not self.google_api_key or not self.google_engine_id:
            logger.error("Google Search API not configured! Please set GOOGLE_SEARCH_API and GOOGLE_SEARCH_ENGINE_ID in .env")
            return []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        queries = self._generate_search_queries(persona)
        logger.info(f"Generated {len(queries)} search queries")
        
        # –ò—â–µ–º –≤ Google –∏ —Å–æ–±–∏—Ä–∞–µ–º URLs –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        urls_to_scrape = await self._search_google(queries)
        logger.info(f"Got {len(urls_to_scrape)} URLs to scrape")
        
        if not urls_to_scrape:
            logger.warning("No URLs found from Google search!")
            return []
        
        # –°–∫—Ä–µ–π–ø–∏–º —Å–∞–π—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        all_channels = await self._scrape_websites(urls_to_scrape)
        logger.info(f"Found {len(all_channels)} UNIQUE channels after scraping")
        
        if not all_channels:
            logger.warning("No channels found via web scraping!")
            return []
        
        # LLM —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_chats = await self._rank_chats_with_llm(persona, list(all_channels.values()))
        
        return ranked_chats[:limit]
    
    def _generate_search_queries(self, persona: Dict[str, Any]) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –ì–†–£–ü–ü–´ (–Ω–µ –∫–∞–Ω–∞–ª—ã).

        Phase 2: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –≥—Ä—É–ø–ø—ã –≥–¥–µ –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å, –∞ –Ω–µ –∫–∞–Ω–∞–ª—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è.
        """

        city = persona.get('city', '–ú–æ—Å–∫–≤–∞')
        interests = persona.get('interests', [])
        occupation = persona.get('occupation', '')

        queries = []

        # –í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢: –ü—É–±–ª–∏—á–Ω—ã–µ –≥—Ä—É–ø–ø—ã –≥–æ—Ä–æ–¥–∞ (–º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å!)
        queries.extend([
            f"{city} telegram –≥—Ä—É–ø–ø–∞ —á–∞—Ç –æ–±—â–µ–Ω–∏–µ",
            f"telegram —á–∞—Ç –∂–∏—Ç–µ–ª–µ–π {city}",
            f"–ø—É–±–ª–∏—á–Ω–∞—è –≥—Ä—É–ø–ø–∞ telegram {city}",
            f"t.me —á–∞—Ç {city} –æ–±—â–µ–Ω–∏–µ",
            f"{city} telegram group chat community",
        ])

        # –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ì–†–£–ü–ü–´ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –≥—Ä—É–ø–ø—ã!)
        for interest in interests[:4]:  # –¢–æ–ø-4 –∏–Ω—Ç–µ—Ä–µ—Å–∞
            queries.extend([
                f"telegram –≥—Ä—É–ø–ø–∞ {interest} {city}",
                f"telegram —á–∞—Ç {interest} –æ–±—Å—É–∂–¥–µ–Ω–∏–µ",
                f"t.me {interest} –≥—Ä—É–ø–ø–∞ —á–∞—Ç",
                f"–ø—É–±–ª–∏—á–Ω—ã–π —á–∞—Ç {interest} telegram",
            ])

        # –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ï –≥—Ä—É–ø–ø—ã
        if occupation:
            queries.extend([
                f"telegram –≥—Ä—É–ø–ø–∞ {occupation}",
                f"telegram —á–∞—Ç {occupation} —Å–æ–æ–±—â–µ—Å—Ç–≤–æ",
            ])

        # –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –≥—Ä—É–ø–ø—ã (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤)
        queries.append(f"telegram –∫–∞–Ω–∞–ª {city} –Ω–æ–≤–æ—Å—Ç–∏")

        return queries[:20]  # –ú–∞–∫—Å 20 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è Phase 2
    
    async def _search_google(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        –ò—â–µ—Ç –≤ Google Custom Search API –∏ —Å–æ–±–∏—Ä–∞–µ—Ç URLs –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å URL –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        """
        urls_to_scrape = []
        seen_urls = set()
        
        for q in queries:
            logger.info(f"Searching Google: {q}")
            
            if not self.google_api_key or not self.google_engine_id:
                continue
            
            params = {
                'key': self.google_api_key,
                'cx': self.google_engine_id,
                'q': q,
                'num': 10  # –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å
            }
            
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(self.google_api_url, params=params)
                    response.raise_for_status()
                    
                    data = response.json()
                    items = data.get('items', [])
                    logger.info(f"  ‚Üí Got {len(items)} results from Google API")
                    
                    for item in items:
                        url = item.get('link', '')
                        if url and url not in seen_urls:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
                            domain = urlparse(url).netloc.lower()
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                            if any(bd in domain for bd in self.blacklisted_domains):
                                logger.debug(f"  ‚úó Skipping blacklisted domain: {domain}")
                                continue
                            
                            urls_to_scrape.append({
                                'url': url,
                                'title': item.get('title', ''),
                                'snippet': item.get('snippet', ''),
                                'displayLink': item.get('displayLink', ''),
                                'query': q
                            })
                            seen_urls.add(url)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã)
                    await asyncio.sleep(0.5)
                    
            except httpx.HTTPStatusError as e:
                logger.error(f"  ‚úó HTTP Error {e.response.status_code} for query '{q}': {e.response.text[:200]}")
                continue
            except Exception as e:
                logger.error(f"  ‚úó Error searching Google for '{q}': {e}")
                continue
        
        return urls_to_scrape
    
    async def _scrape_websites(self, urls: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        –°–∫—Ä–µ–π–ø–∏—Ç —Å–∞–π—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å URL –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            {username: {channel_data}}
        """
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
        urls_to_process = urls[:30]  # –ú–∞–∫—Å 30 —Å–∞–π—Ç–æ–≤
        
        logger.info(f"üåê Scraping {len(urls_to_process)} websites...")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥
        tasks = [self._scrape_single_page(url_data) for url_data in urls_to_process]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
        all_channels = {}
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Error scraping {urls_to_process[i]['url']}: {result}")
                continue
            
            if result:
                for username, channel_data in result.items():
                    if username not in all_channels:
                        all_channels[username] = channel_data
        
        return all_channels
    
    async def _scrape_single_page(self, url_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """
        –°–∫—Ä–µ–π–ø–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã
        
        Args:
            url_data: –°–ª–æ–≤–∞—Ä—å —Å URL –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            {username: {channel_data}}
        """
        url = url_data['url']
        logger.debug(f"  üìÑ Scraping: {url[:80]}...")
        
        try:
            response = await self.http_client.get(url)
            response.raise_for_status()
            
            html = response.text
            logger.debug(f"    ‚úì Loaded {len(html)} chars")
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'lxml')
            
            # –£–¥–∞–ª—è–µ–º script –∏ style —Ç–µ–≥–∏
            for tag in soup(['script', 'style']):
                tag.decompose()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–Ω–∞–ª—ã
            channels = self._extract_channels_from_html(soup, page_text, url_data)
            
            if channels:
                logger.debug(f"    ‚úì Extracted {len(channels)} channels")
            
            return channels
            
        except httpx.TimeoutException:
            logger.debug(f"    ‚úó Timeout: {url[:60]}")
            return {}
        except httpx.HTTPStatusError as e:
            logger.debug(f"    ‚úó HTTP {e.response.status_code}: {url[:60]}")
            return {}
        except Exception as e:
            logger.debug(f"    ‚úó Error: {e}")
            return {}
    
    def _extract_channels_from_html(
        self,
        soup: BeautifulSoup,
        page_text: str,
        url_data: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç Telegram-–∫–∞–Ω–∞–ª—ã –∏–∑ HTML
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
            page_text: –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url_data: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ URL
            
        Returns:
            {username: {channel_data}}
        """
        
        channels = {}
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ username
        blacklist = {
            'gmail', 'mail', 'yandex', 'yahoo', 'outlook', 'hotmail', 'icloud',
            'protonmail', 'aol', 'zoho', 'mailru', 'rambler',
            'instagram', 'facebook', 'twitter', 'tiktok', 'youtube', 'linkedin',
            'whatsapp', 'viber', 'skype', 'discord', 'snapchat',
            'pinterest', 'reddit', 'tumblr', 'flickr', 'vimeo',
            'amazon', 'ebay', 'aliexpress', 'spotify', 'netflix',
            'google', 'microsoft', 'apple', 'samsung', 'huawei',
            'twitch', 'steam', 'playstation', 'xbox', 'nintendo',
            'badoo', 'tinder', 'bumble', 'hinge', 'okcupid',
            'magenta', 'telekom', 'vodafone', 'orange', 't-mobile',
            'katyperry', 'justinbieber', 'arianagrande', 'selenagomez',
            'telegram'  # –û–±—â–∏–π –∫–∞–Ω–∞–ª Telegram
        }
        
        # 1. –ü–†–ò–û–†–ò–¢–ï–¢: –ò—â–µ–º –≤—Å–µ t.me/ —Å—Å—ã–ª–∫–∏ –≤ <a> —Ç–µ–≥–∞—Ö
        telegram_links = soup.find_all('a', href=re.compile(r'(t\.me|telegram\.me)/'))
        
        for link in telegram_links:
            href = link.get('href', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º username
            match = re.search(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', href)
            if match:
                username = match.group(1)
                
                if not self._is_valid_username(username, blacklist):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                link_text = link.get_text(strip=True)
                parent_text = link.find_parent().get_text(strip=True) if link.find_parent() else ''
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': link_text[:100] or url_data.get('title', '')[:100] or username,
                        'description': parent_text[:200] or url_data.get('snippet', '')[:200],
                        'source_url': url_data['url'],
                        'confidence': 'high'
                    }
        
        # 2. –ò—â–µ–º t.me/ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
        text_links = re.findall(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', page_text)
        
        for username in set(text_links):  # set() –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            if not self._is_valid_username(username, blacklist):
                continue
            
            if username not in channels:
                channels[username] = {
                    'username': f"@{username}",
                    'title': username,
                    'description': url_data.get('snippet', '')[:200],
                    'source_url': url_data['url'],
                    'confidence': 'medium'
                }
        
        # 3. –ò—â–µ–º @username —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å Telegram –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        page_text_lower = page_text.lower()
        has_telegram_context = any(
            keyword in page_text_lower
            for keyword in ['telegram', 't.me', '—Ç–µ–ª–µ–≥—Ä–∞–º', '—Ç–µ–ª–µ–≥—Ä–∞–º–º', '—Ç–µ–ª–µ–≥–∞']
        )
        
        if has_telegram_context:
            mentions = re.findall(r'@([a-zA-Z][a-zA-Z0-9_]{4,31})', page_text)
            
            for username in set(mentions):
                if not self._is_valid_username(username, blacklist):
                    continue
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': username,
                        'description': url_data.get('snippet', '')[:200],
                        'source_url': url_data['url'],
                        'confidence': 'low'
                    }
        
        return channels
    
    def _is_valid_username(self, username: str, blacklist: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å Telegram username"""
        
        username_lower = username.lower()
        
        if username_lower in blacklist:
            return False
        
        if len(username) < 5 or len(username) > 32:
            return False
        
        if '.' in username or '___' in username:
            return False
        
        if username.endswith('_') or username.startswith('_'):
            return False
        
        if re.search(r'\d{3,}$', username):
            return False
        
        if not username[0].isalpha():
            return False
        
        return True
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        await self.http_client.aclose()
    
    async def _rank_chats_with_llm(
        self,
        persona: Dict[str, Any],
        channels: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
        
        if not channels:
            return []
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è LLM (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ = –¥–æ—Ä–æ–≥–æ)
        channels_for_llm = channels[:30]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM
        channels_list = "\n".join([
            f"{i+1}. {ch['username']} - {ch['title']} ({ch.get('description', '')[:80]})"
            for i, ch in enumerate(channels_for_llm)
        ])
        
        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Telegram. –û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤/–≥—Ä—É–ø–ø –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
- –ì–æ—Ä–æ–¥: {persona.get('city')}
- –ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(persona.get('interests', []))}
- –ü—Ä–æ—Ñ–µ—Å—Å–∏—è: {persona.get('occupation')}
- –í–æ–∑—Ä–∞—Å—Ç: {persona.get('age')}

–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã:
{channels_list}

–û—Ü–µ–Ω–∏ –ö–ê–ñ–î–´–ô –∫–∞–Ω–∞–ª –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0.0-1.0):
- 1.0 = –ò–î–ï–ê–õ–¨–ù–û –ø–æ–¥—Ö–æ–¥–∏—Ç (–≥—Ä—É–ø–ø–∞ –≥–æ—Ä–æ–¥–∞, —Ç–æ—á–Ω–æ–µ –ø–æ–ø–∞–¥–∞–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º)
- 0.8 = –û—Ç–ª–∏—á–Ω–æ (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä—É–ø–ø–∞ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º)
- 0.5 = –°—Ä–µ–¥–Ω–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ)
- 0.3 = –°–ª–∞–±–æ (–∫–æ—Å–≤–µ–Ω–Ω–∞—è —Å–≤—è–∑—å)
- 0.0 = –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç

–ü–†–ò–û–†–ò–¢–ï–¢–´:
1. –ì—Ä—É–ø–ø—ã/—á–∞—Ç—ã –ì–û–†–û–î–ê –¥–ª—è –æ–±—â–µ–Ω–∏—è (group/supergroup) - –í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢!
2. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã –ø–æ –ò–ù–¢–ï–†–ï–°–ê–ú –≤ –≥–æ—Ä–æ–¥–µ
3. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
4. –û–±—â–∏–µ –∫–∞–Ω–∞–ª—ã –≥–æ—Ä–æ–¥–∞ (–Ω–æ–≤–æ—Å—Ç–∏)

–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø (group/channel/supergroup) - –≥—Ä—É–ø–ø—ã –ª—É—á—à–µ —á–µ–º –∫–∞–Ω–∞–ª—ã!

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - JSON –º–∞—Å—Å–∏–≤:
[
  {{
    "username": "@example",
    "relevance_score": 0.9,
    "chat_type": "group",
    "reason": "–ì—Ä—É–ø–ø–∞ –≥–æ—Ä–æ–¥–∞ –¥–ª—è –æ–±—â–µ–Ω–∏—è"
  }},
  ...
]

–¢–û–õ–¨–ö–û JSON!"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                temperature=0.3,
                max_tokens=3000,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Telegram. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            response_text = response.choices[0].message.content
            
            # Parse JSON
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0].strip()
            else:
                json_str = response_text.strip()
            
            rankings = json.loads(json_str)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            username_to_channel = {ch['username']: ch for ch in channels_for_llm}
            
            ranked_chats = []
            for rank in rankings:
                username = rank.get('username', '')
                if username in username_to_channel:
                    original = username_to_channel[username]
                    ranked_chats.append({
                        "chat_username": username,
                        "chat_title": original.get('title', ''),
                        "chat_description": original.get('description', ''),
                        "chat_type": rank.get('chat_type', 'unknown'),
                        "member_count": None,
                        "relevance_score": rank.get('relevance_score', 0.5),
                        "relevance_reason": rank.get('reason', '')
                    })
            
            logger.info(f"LLM ranked {len(ranked_chats)} chats")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-5
            for i, chat in enumerate(sorted(ranked_chats, key=lambda x: x['relevance_score'], reverse=True)[:5]):
                logger.info(f"  {i+1}. {chat['chat_username']} ({chat['chat_type']}, score: {chat['relevance_score']:.2f})")
            
            return ranked_chats
            
        except Exception as e:
            logger.error(f"Error ranking chats: {e}")
            
            # Fallback - –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–Ω–∞–ª—ã –∫–∞–∫ –µ—Å—Ç—å
            return [{
                "chat_username": ch['username'],
                "chat_title": ch.get('title', ''),
                "chat_description": ch.get('description', ''),
                "chat_type": "unknown",
                "member_count": None,
                "relevance_score": 0.5,
                "relevance_reason": "Found via search"
            } for ch in channels_for_llm]
