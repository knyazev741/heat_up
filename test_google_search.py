"""
–¢–µ—Å—Ç Google Custom Search API —Å –ø–µ—Ä–µ—Ö–æ–¥–æ–º –Ω–∞ —Å–∞–π—Ç—ã –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º Telegram —Å—Å—ã–ª–æ–∫

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
1. –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ Google API
2. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (URLs, titles, snippets)
3. –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –∫–∞–∂–¥—ã–π —Å–∞–π—Ç
4. –ü–∞—Ä—Å–∏–Ω–≥ HTML
5. –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ Telegram —Å—Å—ã–ª–∫–∏
"""

import asyncio
import logging
import sys
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
Path("logs").mkdir(exist_ok=True)

log_file = f'logs/google_search_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_file, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥–∞
from config import settings


class GoogleSearchTester:
    """–¢–µ—Å—Ç–µ—Ä Google Custom Search API —Å web scraping"""
    
    def __init__(self):
        self.api_key = settings.google_search_api_key
        self.engine_id = settings.google_search_engine_id
        self.api_url = "https://www.googleapis.com/customsearch/v1"
        
        if not self.api_key:
            raise ValueError("GOOGLE_SEARCH_API_KEY not set in .env")
        if not self.engine_id:
            raise ValueError("GOOGLE_SEARCH_ENGINE_ID not set in .env")
        
        # HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        self.http_client = httpx.AsyncClient(
            timeout=15.0,
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
        )
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤
        self.blacklisted_domains = {
            'instagram.com', 'facebook.com', 'twitter.com', 'x.com',
            'youtube.com', 'tiktok.com', 'linkedin.com', 'reddit.com',
            'pinterest.com', 'amazon.com', 'ebay.com'
        }
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ username
        self.blacklisted_usernames = {
            'gmail', 'mail', 'yandex', 'yahoo', 'outlook', 'hotmail', 'icloud',
            'instagram', 'facebook', 'twitter', 'tiktok', 'youtube', 'linkedin',
            'magenta', 'telekom', 'katyperry', 'justinbieber'
        }
    
    async def search_google(self, query: str, num_results: int = 10) -> List[Dict[str, Any]]:
        """
        –ò—â–µ—Ç –≤ Google Custom Search API
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–º–∞–∫—Å 10 –∑–∞ –∑–∞–ø—Ä–æ—Å)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        logger.info(f"\n{'='*100}")
        logger.info(f"üîç GOOGLE SEARCH: {query}")
        logger.info(f"{'='*100}")
        
        params = {
            'key': self.api_key,
            'cx': self.engine_id,
            'q': query,
            'num': min(num_results, 10)  # Google API –º–∞–∫—Å–∏–º—É–º 10 –∑–∞ –∑–∞–ø—Ä–æ—Å
        }
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(self.api_url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                items = data.get('items', [])
                logger.info(f"‚úÖ Got {len(items)} results from Google API")
                
                results = []
                for i, item in enumerate(items, 1):
                    result = {
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'displayLink': item.get('displayLink', '')
                    }
                    results.append(result)
                    
                    logger.info(f"\n  Result {i}:")
                    logger.info(f"    Title:  {result['title'][:80]}")
                    logger.info(f"    URL:    {result['link'][:80]}")
                    logger.info(f"    Domain: {result['displayLink']}")
                    logger.info(f"    Snippet: {result['snippet'][:100]}")
                
                return results
                
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ùå HTTP Error: {e.response.status_code}")
            logger.error(f"   Response: {e.response.text[:500]}")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error searching Google: {e}")
            return []
    
    async def scrape_website(self, url: str) -> Dict[str, Any]:
        """
        –°–∫—Ä–µ–π–ø–∏—Ç –æ–¥–∏–Ω —Å–∞–π—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç Telegram —Å—Å—ã–ª–∫–∏
        
        Returns:
            {
                'url': url,
                'success': bool,
                'channels_found': int,
                'channels': List[Dict]
            }
        """
        logger.info(f"\n  üìÑ Scraping: {url[:80]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
        domain = urlparse(url).netloc.lower()
        if any(bd in domain for bd in self.blacklisted_domains):
            logger.info(f"    ‚úó Skipping blacklisted domain: {domain}")
            return {
                'url': url,
                'success': False,
                'reason': 'blacklisted_domain',
                'channels_found': 0,
                'channels': []
            }
        
        try:
            response = await self.http_client.get(url)
            response.raise_for_status()
            
            html = response.text
            logger.info(f"    ‚úì Loaded {len(html)} chars")
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'lxml')
            
            # –£–¥–∞–ª—è–µ–º script –∏ style
            for tag in soup(['script', 'style']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–Ω–∞–ª—ã
            channels = self._extract_telegram_channels(soup, html, url)
            
            logger.info(f"    ‚úì Found {len(channels)} Telegram channels")
            
            if channels:
                for ch in channels[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    logger.info(f"      ‚Ä¢ {ch['username']:30s} - {ch.get('title', '')[:50]}")
                if len(channels) > 5:
                    logger.info(f"      ... and {len(channels) - 5} more")
            
            return {
                'url': url,
                'success': True,
                'channels_found': len(channels),
                'channels': channels
            }
            
        except httpx.TimeoutException:
            logger.warning(f"    ‚úó Timeout")
            return {
                'url': url,
                'success': False,
                'reason': 'timeout',
                'channels_found': 0,
                'channels': []
            }
        except httpx.HTTPStatusError as e:
            logger.warning(f"    ‚úó HTTP {e.response.status_code}")
            return {
                'url': url,
                'success': False,
                'reason': f'http_{e.response.status_code}',
                'channels_found': 0,
                'channels': []
            }
        except Exception as e:
            logger.warning(f"    ‚úó Error: {e}")
            return {
                'url': url,
                'success': False,
                'reason': str(e),
                'channels_found': 0,
                'channels': []
            }
    
    def _extract_telegram_channels(
        self,
        soup: BeautifulSoup,
        html: str,
        source_url: str
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç Telegram –∫–∞–Ω–∞–ª—ã –∏–∑ HTML"""
        
        channels = {}
        
        # 1. –ü–†–ò–û–†–ò–¢–ï–¢: –ò—â–µ–º –≤—Å–µ t.me/ —Å—Å—ã–ª–∫–∏ –≤ <a> —Ç–µ–≥–∞—Ö
        telegram_links = soup.find_all('a', href=re.compile(r'(t\.me|telegram\.me)/'))
        
        for link in telegram_links:
            href = link.get('href', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º username
            match = re.search(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', href)
            if match:
                username = match.group(1)
                
                if not self._is_valid_username(username):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                link_text = link.get_text(strip=True)
                parent_text = link.parent.get_text(strip=True) if link.parent else ''
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': link_text[:100] or username,
                        'description': parent_text[:200],
                        'source_url': source_url,
                        'confidence': 'high'
                    }
        
        # 2. –ò—â–µ–º t.me/ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
        text_links = re.findall(r'(?:t\.me|telegram\.me)/([a-zA-Z0-9_]+)', html)
        
        for username in text_links:
            if not self._is_valid_username(username):
                continue
            
            if username not in channels:
                channels[username] = {
                    'username': f"@{username}",
                    'title': username,
                    'description': '',
                    'source_url': source_url,
                    'confidence': 'medium'
                }
        
        # 3. –ò—â–µ–º @username —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å Telegram –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        html_lower = html.lower()
        has_telegram_context = any(
            keyword in html_lower
            for keyword in ['telegram', 't.me', '—Ç–µ–ª–µ–≥—Ä–∞–º', '—Ç–µ–ª–µ–≥—Ä–∞–º–º', '—Ç–µ–ª–µ–≥–∞']
        )
        
        if has_telegram_context:
            mentions = re.findall(r'@([a-zA-Z][a-zA-Z0-9_]{4,31})', html)
            
            for username in set(mentions):
                if not self._is_valid_username(username):
                    continue
                
                if username not in channels:
                    channels[username] = {
                        'username': f"@{username}",
                        'title': username,
                        'description': '',
                        'source_url': source_url,
                        'confidence': 'low'
                    }
        
        return list(channels.values())
    
    def _is_valid_username(self, username: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å Telegram username"""
        
        username_lower = username.lower()
        
        if username_lower in self.blacklisted_usernames:
            return False
        
        if len(username) < 5 or len(username) > 32:
            return False
        
        if '.' in username or '___' in username:
            return False
        
        if username.endswith('_') or username.startswith('_'):
            return False
        
        if re.search(r'\d{3,}$', username):
            return False
        
        if not username[0].isalpha():
            return False
        
        return True
    
    async def test_full_flow(self, queries: List[str]):
        """
        –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç: –ø–æ–∏—Å–∫ ‚Üí —Å–∫—Ä–µ–π–ø–∏–Ω–≥ ‚Üí –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        logger.info("=" * 100)
        logger.info("üöÄ GOOGLE SEARCH API TEST WITH WEB SCRAPING")
        logger.info("=" * 100)
        logger.info(f"\nAPI Key: {self.api_key[:10]}...")
        logger.info(f"Engine ID: {self.engine_id[:10]}...")
        logger.info(f"Queries to test: {len(queries)}")
        
        all_channels = {}
        all_scrape_results = []
        
        # 1. –ò—â–µ–º –≤ Google
        for query in queries:
            search_results = await self.search_google(query, num_results=5)
            
            if not search_results:
                logger.warning(f"  ‚ö†Ô∏è No results for query: {query}")
                continue
            
            # 2. –°–∫—Ä–µ–π–ø–∏–º –∫–∞–∂–¥—ã–π —Å–∞–π—Ç
            logger.info(f"\n  üåê Scraping {len(search_results)} websites...")
            
            scrape_tasks = [self.scrape_website(r['link']) for r in search_results]
            scrape_results = await asyncio.gather(*scrape_tasks, return_exceptions=True)
            
            for i, result in enumerate(scrape_results):
                if isinstance(result, Exception):
                    logger.error(f"    ‚úó Exception: {result}")
                    continue
                
                all_scrape_results.append({
                    'query': query,
                    'search_result': search_results[i],
                    'scrape_result': result
                })
                
                # –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–∞–ª—ã
                for ch in result.get('channels', []):
                    username = ch['username']
                    if username not in all_channels:
                        all_channels[username] = ch
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã)
            await asyncio.sleep(1)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        logger.info("\n" + "=" * 100)
        logger.info("üìä FINAL REPORT")
        logger.info("=" * 100)
        
        logger.info(f"\nQueries tested: {len(queries)}")
        logger.info(f"Websites scraped: {len(all_scrape_results)}")
        logger.info(f"Successful scrapes: {sum(1 for r in all_scrape_results if r['scrape_result'].get('success'))}")
        logger.info(f"Total channels found: {len(all_channels)}")
        
        if all_channels:
            logger.info("\n‚úÖ All found Telegram channels:")
            logger.info("=" * 100)
            for i, (username, ch) in enumerate(sorted(all_channels.items()), 1):
                logger.info(
                    f"{i:3d}. {ch['username']:30s} | "
                    f"Confidence: {ch['confidence']:6s} | "
                    f"{ch.get('title', '')[:50]}"
                )
        else:
            logger.warning("‚ö†Ô∏è No Telegram channels found!")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report = {
            'timestamp': datetime.now().isoformat(),
            'queries': queries,
            'scrape_results': all_scrape_results,
            'channels': list(all_channels.values()),
            'summary': {
                'queries_tested': len(queries),
                'websites_scraped': len(all_scrape_results),
                'successful_scrapes': sum(1 for r in all_scrape_results if r['scrape_result'].get('success')),
                'channels_found': len(all_channels)
            }
        }
        
        report_file = f"logs/google_search_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nüíæ Detailed report saved to: {report_file}")
        logger.info(f"üìÑ Full log saved to: {log_file}")
        
        return all_channels
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        await self.http_client.aclose()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∞"""
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ö–∞–∑–∞–Ω—å telegram –≥—Ä—É–ø–ø–∞",
        "telegram —á–∞—Ç –ö–∞–∑–∞–Ω—å –æ–±—â–µ–Ω–∏–µ",
        "t.me kazan chat"
    ]
    
    try:
        tester = GoogleSearchTester()
        
        await tester.test_full_flow(test_queries)
        
    except ValueError as e:
        logger.error(f"‚ùå Configuration error: {e}")
        logger.error("\nPlease set in .env:")
        logger.error("  GOOGLE_SEARCH_API=your_api_key")
        logger.error("  GOOGLE_SEARCH_ENGINE_ID=your_engine_id")
        logger.error("\nTo get Engine ID:")
        logger.error("  1. Go to https://programmablesearchengine.google.com/")
        logger.error("  2. Create a new search engine")
        logger.error("  3. Set 'Sites to search' to 'Search the entire web'")
        logger.error("  4. Copy the 'Search engine ID' (cx parameter)")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error: {e}", exc_info=True)
        sys.exit(1)
    finally:
        if 'tester' in locals():
            await tester.close()


if __name__ == "__main__":
    asyncio.run(main())

