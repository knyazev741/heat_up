"""
Test LLM prompt with synthetic data to verify action generation.

This test:
1. Creates synthetic data matching real warmup conditions
2. Tests reply_to_dm action generation with pending DMs
3. Tests group chat actions (reply_in_chat)
4. Verifies LLM produces expected action types

Run: python tests/test_llm_prompt_actions.py
"""

import asyncio
import sys
import os
import json
import logging

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_agent import ActionPlannerAgent
from config import settings

# LLM Configuration
LLM_CONFIG = {
    "api_key": settings.deepseek_api_key,
    "base_url": "https://api.deepseek.com",
    "model": "deepseek-chat"
}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Synthetic data matching real warmup conditions
SYNTHETIC_ACCOUNT = {
    "id": 999,
    "session_id": "99999",
    "warmup_stage": 5,  # Stage 5+ allows reactions, bots, reply_to_dm
    "phone_number": "+79001234567",
    "is_active": True,
    "account_type": "warmup"
}

SYNTHETIC_PERSONA = {
    "generated_name": "–ú–∞—Ä–∏—è –°–æ–∫–æ–ª–æ–≤–∞",
    "age": 28,
    "occupation": "–¥–∏–∑–∞–π–Ω–µ—Ä –∏–Ω—Ç–µ—Ä—å–µ—Ä–æ–≤",
    "city": "–ö–∞–∑–∞–Ω—å",
    "country": "–†–æ—Å—Å–∏—è",
    "interests": ["–¥–∏–∑–∞–π–Ω", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è", "–∫—É–ª–∏–Ω–∞—Ä–∏—è", "–π–æ–≥–∞"],
    "personality_traits": ["–∫—Ä–µ–∞—Ç–∏–≤–Ω–∞—è", "–æ–±—â–∏—Ç–µ–ª—å–Ω–∞—è", "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–∞—è –∫ –¥–µ—Ç–∞–ª—è–º"],
    "communication_style": "–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π",
    "activity_level": "—Å—Ä–µ–¥–Ω–∏–π",
    "full_description": "–ú–∞—Ä–∏—è - —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä –∏–Ω—Ç–µ—Ä—å–µ—Ä–æ–≤ –∏–∑ –ö–∞–∑–∞–Ω–∏. –õ—é–±–∏—Ç –ø—É—Ç–µ—à–µ—Å—Ç–≤–æ–≤–∞—Ç—å –∏ –Ω–∞—Ö–æ–¥–∏—Ç—å –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω.",
    "background_story": "–ó–∞–∫–æ–Ω—á–∏–ª–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Ñ–∞–∫—É–ª—å—Ç–µ—Ç, —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Å—Ç—É–¥–∏–∏ –¥–∏–∑–∞–π–Ω–∞ —É–∂–µ 5 –ª–µ—Ç."
}

SYNTHETIC_PENDING_DMS = [
    {
        "conversation_id": 42,
        "sender_name": "–ê–ª–µ–∫—Å–µ–π –ü–µ—Ç—Ä–æ–≤",
        "last_message_text": "–ü—Ä–∏–≤–µ—Ç! –í–∏–¥–µ–ª–∞ —Ç–≤–æ–∏ —Ä–∞–±–æ—Ç—ã –ø–æ –¥–∏–∑–∞–π–Ω—É, –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å! –ú–æ–∂–µ—à—å —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Å–≤–æ—ë–º —Å—Ç–∏–ª–µ?",
        "peer_session_id": "12345"
    },
    {
        "conversation_id": 43,
        "sender_name": "–ò—Ä–∏–Ω–∞ –í–æ–ª–∫–æ–≤–∞",
        "last_message_text": "–ü—Ä–∏–≤–µ—Ç, –ú–∞—Ä–∏—è! –ö–∞–∫ –¥–µ–ª–∞? –î–∞–≤–Ω–æ –Ω–µ –æ–±—â–∞–ª–∏—Å—å. –¢—ã –≤—Å—ë –µ—â—ë –≤ –ö–∞–∑–∞–Ω–∏?",
        "peer_session_id": "12346"
    }
]

SYNTHETIC_RELEVANT_CHATS = [
    {"chat_username": "@design_kazan", "chat_title": "–î–∏–∑–∞–π–Ω –∏–Ω—Ç–µ—Ä—å–µ—Ä–æ–≤ –ö–∞–∑–∞–Ω—å", "chat_type": "group", "is_joined": True, "relevance_score": 0.95, "relevance_reason": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –¥–∏–∑–∞–π–Ω–µ—Ä–æ–≤"},
    {"chat_username": "@travel_russia", "chat_title": "–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è –ø–æ –†–æ—Å—Å–∏–∏", "chat_type": "channel", "is_joined": True, "relevance_score": 0.85, "relevance_reason": "–ò–Ω—Ç–µ—Ä–µ—Å –∫ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è–º"},
    {"chat_username": "@yoga_kazan", "chat_title": "–ô–æ–≥–∞ –ö–∞–∑–∞–Ω—å", "chat_type": "group", "is_joined": False, "relevance_score": 0.80, "relevance_reason": "–ò–Ω—Ç–µ—Ä–µ—Å –∫ –π–æ–≥–µ"},
    {"chat_username": "@architecture_news", "chat_title": "–ù–æ–≤–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "chat_type": "channel", "is_joined": True, "relevance_score": 0.75, "relevance_reason": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å"},
    {"chat_username": "@cooking_recipes", "chat_title": "–†–µ—Ü–µ–ø—Ç—ã –∏ –∫—É–ª–∏–Ω–∞—Ä–∏—è", "chat_type": "channel", "is_joined": False, "relevance_score": 0.70, "relevance_reason": "–ò–Ω—Ç–µ—Ä–µ—Å –∫ –∫—É–ª–∏–Ω–∞—Ä–∏–∏"},
]


def build_test_prompt_with_pending_dms() -> str:
    """Build prompt with synthetic pending DMs to test reply_to_dm generation."""

    persona = SYNTHETIC_PERSONA
    account = SYNTHETIC_ACCOUNT
    pending_dms = SYNTHETIC_PENDING_DMS
    chats = SYNTHETIC_RELEVANT_CHATS

    # Build pending DMs context (same as in llm_agent.py)
    dm_lines = []
    for dm in pending_dms:
        sender_name = dm.get('sender_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
        last_msg = dm.get('last_message_text', '')[:100]
        conv_id = dm.get('conversation_id')
        dm_lines.append(f"- –û—Ç {sender_name} (conversation_id={conv_id}): \"{last_msg}...\"")

    pending_dms_context = f"""
üì¨ –ù–ï–ü–†–û–ß–ò–¢–ê–ù–ù–´–ï –°–û–û–ë–©–ï–ù–ò–Ø –í –õ–°:
–£ —Ç–µ–±—è –µ—Å—Ç—å –Ω–µ–ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π! –¢—ã –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–∏—Ö, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å.

{chr(10).join(dm_lines)}

–ß—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–π –¥–µ–π—Å—Ç–≤–∏–µ reply_to_dm —Å conversation_id.
"""

    # Build channels list
    channels_list = "\n".join([
        f"- {ch['chat_username']}: {ch.get('chat_title', 'Unknown')} "
        f"[{(ch.get('chat_type') or 'unknown').upper()}] "
        f"{'[–í–°–¢–£–ü–ò–õ ‚úÖ]' if ch.get('is_joined') else '[–ù–û–í–´–ô]'} "
        f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {ch.get('relevance_score', 0):.1f}) - {ch.get('relevance_reason', '')[:80]}"
        for ch in chats
    ])

    prompt = f"""–¢–´ - —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å Telegram —Å–æ —Å–ª–µ–¥—É—é—â–µ–π –ª–∏—á–Ω–æ—Å—Ç—å—é:

–ò–º—è: {persona['generated_name']}
–í–æ–∑—Ä–∞—Å—Ç: {persona['age']} –ª–µ—Ç
–ü—Ä–æ—Ñ–µ—Å—Å–∏—è: {persona['occupation']}
–ì–æ—Ä–æ–¥: {persona['city']}, {persona['country']}
–ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(persona['interests'])}
–•–∞—Ä–∞–∫—Ç–µ—Ä: {', '.join(persona['personality_traits'])}
–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è: {persona['communication_style']}
–£—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {persona['activity_level']}

–û —Ç–µ–±–µ:
{persona['full_description']}

üìÖ –¢–ï–ö–£–©–ê–Ø –°–¢–ê–î–ò–Ø –ü–†–û–ì–†–ï–í–ê: –î–µ–Ω—å {account['warmup_stage']}

–õ–ò–ú–ò–¢–´ –î–õ–Ø –≠–¢–û–ô –°–¢–ê–î–ò–ò:
- –ú–∞–∫—Å–∏–º—É–º –¥–µ–π—Å—Ç–≤–∏–π: 15
- –ú–∞–∫—Å–∏–º—É–º –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π –≤ –Ω–æ–≤—ã–µ —á–∞—Ç—ã: 3
- –ú–∞–∫—Å–∏–º—É–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: 2

{pending_dms_context}

üìã –î–û–°–¢–£–ü–ù–´–ï –ß–ê–¢–´/–ö–ê–ù–ê–õ–´ (–ø–æ–¥–æ–±—Ä–∞–Ω—ã –°–ü–ï–¶–ò–ê–õ–¨–ù–û –¥–ª—è –¢–í–û–ò–• –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤):
{channels_list}

–î–û–°–¢–£–ü–ù–´–ï –¢–ò–ü–´ –î–ï–ô–°–¢–í–ò–ô:

1. join_channel (join_chat):
   {{"action": "join_channel", "channel_username": "@example", "reason": "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞"}}

2. read_messages:
   {{"action": "read_messages", "channel_username": "@example", "duration_seconds": 15, "reason": "–ß–∏—Ç–∞—é –∫–æ–Ω—Ç–µ–Ω—Ç"}}

3. idle:
   {{"action": "idle", "duration_seconds": 7, "reason": "–ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞"}}

4. view_profile:
   {{"action": "view_profile", "channel_username": "@example", "duration_seconds": 5, "reason": "–ò–∑—É—á–∞—é —á–∞—Ç/–∫–∞–Ω–∞–ª"}}

5. "react_to_message" - –ü–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∞–∫—Ü–∏—é –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ
   - Params: channel_username (–∏–ª–∏ chat_username)

6. "reply_to_dm" - –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –ª–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
   - Params: conversation_id, message (—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞)
   - –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –õ–° (—Å–º. –≤—ã—à–µ)
   - –ü—Ä–∏–º–µ—Ä: {{"action": "reply_to_dm", "conversation_id": 123, "message": "–ü—Ä–∏–≤–µ—Ç! –î–∞, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Ç–µ–º–∞..."}}

7. "reply_in_chat" - –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –≥—Ä—É–ø–ø–µ
   - Params: chat_username, reply_text

‚ö†Ô∏è –í–ê–ñ–ù–û: –£ —Ç–µ–±—è –µ—Å—Ç—å –Ω–µ–ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –õ–°! –û–±—Ä–∞—Ç–∏ –Ω–∞ –Ω–∏—Ö –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—å –µ—Å–ª–∏ —Ö–æ—á–µ—à—å.

–°—Ç–∞–¥–∏—è: {account['warmup_stage']}
–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞!"""

    return prompt


def build_test_prompt_for_group_reply() -> str:
    """Build prompt to test reply_in_chat for group messages."""

    persona = SYNTHETIC_PERSONA
    account = SYNTHETIC_ACCOUNT.copy()
    account["warmup_stage"] = 10  # Stage 10+ for reply_in_chat
    chats = SYNTHETIC_RELEVANT_CHATS

    channels_list = "\n".join([
        f"- {ch['chat_username']}: {ch.get('chat_title', 'Unknown')} "
        f"[{(ch.get('chat_type') or 'unknown').upper()}] "
        f"{'[–í–°–¢–£–ü–ò–õ ‚úÖ]' if ch.get('is_joined') else '[–ù–û–í–´–ô]'} "
        f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {ch.get('relevance_score', 0):.1f})"
        for ch in chats
    ])

    # Simulate recent group messages that could trigger reply
    group_context = """
üì¢ –ü–û–°–õ–ï–î–ù–ò–ï –°–û–û–ë–©–ï–ù–ò–Ø –í –ì–†–£–ü–ü–ê–•, –ì–î–ï –¢–´ –£–ß–ê–°–¢–í–£–ï–®–¨:

@design_kazan (–î–∏–∑–∞–π–Ω –∏–Ω—Ç–µ—Ä—å–µ—Ä–æ–≤ –ö–∞–∑–∞–Ω—å):
- –ê–Ω–Ω–∞: "–ö—Ç–æ-–Ω–∏–±—É–¥—å —Ä–∞–±–æ—Ç–∞–ª —Å –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–º –∫–∞–º–Ω–µ–º –≤ –∏–Ω—Ç–µ—Ä—å–µ—Ä–µ? –ò—â—É —Å–æ–≤–µ—Ç—ã"
- –î–º–∏—Ç—Ä–∏–π: "–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è—é—Å—å –∫ –≤–æ–ø—Ä–æ—Å—É, —Ç–æ–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –≥—Ä—É–ø–ø–µ, –∏—Å–ø–æ–ª—å–∑—É–π reply_in_chat.
"""

    prompt = f"""–¢–´ - —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å Telegram —Å–æ —Å–ª–µ–¥—É—é—â–µ–π –ª–∏—á–Ω–æ—Å—Ç—å—é:

–ò–º—è: {persona['generated_name']}
–í–æ–∑—Ä–∞—Å—Ç: {persona['age']} –ª–µ—Ç
–ü—Ä–æ—Ñ–µ—Å—Å–∏—è: {persona['occupation']}
–ì–æ—Ä–æ–¥: {persona['city']}, {persona['country']}
–ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(persona['interests'])}

üìÖ –¢–ï–ö–£–©–ê–Ø –°–¢–ê–î–ò–Ø –ü–†–û–ì–†–ï–í–ê: –î–µ–Ω—å {account['warmup_stage']}

–õ–ò–ú–ò–¢–´ –î–õ–Ø –≠–¢–û–ô –°–¢–ê–î–ò–ò:
- –ú–∞–∫—Å–∏–º—É–º –¥–µ–π—Å—Ç–≤–∏–π: 20
- –ú–∞–∫—Å–∏–º—É–º –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π –≤ –Ω–æ–≤—ã–µ —á–∞—Ç—ã: 5
- –ú–∞–∫—Å–∏–º—É–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: 5

{group_context}

üìã –î–û–°–¢–£–ü–ù–´–ï –ß–ê–¢–´/–ö–ê–ù–ê–õ–´:
{channels_list}

–î–û–°–¢–£–ü–ù–´–ï –¢–ò–ü–´ –î–ï–ô–°–¢–í–ò–ô:

1. read_messages:
   {{"action": "read_messages", "channel_username": "@example", "duration_seconds": 15}}

2. idle:
   {{"action": "idle", "duration_seconds": 7, "reason": "–ü–∞—É–∑–∞"}}

3. "react_to_message" - –ü–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∞–∫—Ü–∏—é
   - Params: channel_username

4. "reply_in_chat" - –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –≥—Ä—É–ø–ø–µ
   - Params: chat_username, reply_text
   - –ü—Ä–∏–º–µ—Ä: {{"action": "reply_in_chat", "chat_username": "@design_kazan", "reply_text": "–ü—Ä–∏–≤–µ—Ç! –î–∞, —Ä–∞–±–æ—Ç–∞–ª–∞ —Å –∫–∞–º–Ω–µ–º..."}}

‚ö†Ô∏è –í–ê–ñ–ù–û: –¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä. –í –≥—Ä—É–ø–ø–µ @design_kazan —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –ø—Ä–æ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π –∫–∞–º–µ–Ω—å - —ç—Ç–æ —Ç–≤–æ—è —Ç–µ–º–∞! –ú–æ–∂–µ—à—å –ø–æ–º–æ—á—å —Å–æ–≤–µ—Ç–æ–º.

–°—Ç–∞–¥–∏—è: {account['warmup_stage']}
–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞!"""

    return prompt


async def test_reply_to_dm_generation():
    """Test that LLM generates reply_to_dm action when pending DMs exist."""
    logger.info("=" * 80)
    logger.info("TEST 1: reply_to_dm action generation")
    logger.info("=" * 80)

    prompt = build_test_prompt_with_pending_dms()
    logger.info(f"Prompt length: {len(prompt)} chars")
    logger.info(f"Pending DMs in prompt: {len(SYNTHETIC_PENDING_DMS)}")

    # Use DeepSeek directly
    import httpx

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{LLM_CONFIG['base_url']}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLM_CONFIG['api_key']}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": LLM_CONFIG["model"],
                    "messages": [
                        {"role": "system", "content": "–¢—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è Telegram. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤–æ–º."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 2000
                }
            )

            result = response.json()
            content = result["choices"][0]["message"]["content"]

            # Parse JSON
            content = content.strip()
            if content.startswith("```"):
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]

            actions = json.loads(content)

            logger.info(f"\nüìã Generated {len(actions)} actions:")

            has_reply_to_dm = False
            for i, action in enumerate(actions, 1):
                action_type = action.get("action", "unknown")
                logger.info(f"  {i}. {action_type}")
                if action_type == "reply_to_dm":
                    has_reply_to_dm = True
                    logger.info(f"     ‚úÖ FOUND reply_to_dm!")
                    logger.info(f"     conversation_id: {action.get('conversation_id')}")
                    logger.info(f"     message: {action.get('message', '')[:50]}...")

            if has_reply_to_dm:
                logger.info("\n‚úÖ TEST PASSED: LLM generated reply_to_dm action")
                return True
            else:
                logger.warning("\n‚ö†Ô∏è TEST WARNING: LLM did NOT generate reply_to_dm action")
                logger.info("This may be normal - LLM has freedom to choose actions")
                return False

    except Exception as e:
        logger.error(f"Test failed with error: {e}")
        return False


async def test_reply_in_chat_generation():
    """Test that LLM generates reply_in_chat action for group messages."""
    logger.info("\n" + "=" * 80)
    logger.info("TEST 2: reply_in_chat action generation (group replies)")
    logger.info("=" * 80)

    prompt = build_test_prompt_for_group_reply()
    logger.info(f"Prompt length: {len(prompt)} chars")

    import httpx

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{LLM_CONFIG['base_url']}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLM_CONFIG['api_key']}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": LLM_CONFIG["model"],
                    "messages": [
                        {"role": "system", "content": "–¢—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è Telegram. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤–æ–º."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 2000
                }
            )

            result = response.json()
            content = result["choices"][0]["message"]["content"]

            # Parse JSON
            content = content.strip()
            if content.startswith("```"):
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]

            actions = json.loads(content)

            logger.info(f"\nüìã Generated {len(actions)} actions:")

            has_reply_in_chat = False
            for i, action in enumerate(actions, 1):
                action_type = action.get("action", "unknown")
                logger.info(f"  {i}. {action_type}")
                if action_type == "reply_in_chat":
                    has_reply_in_chat = True
                    logger.info(f"     ‚úÖ FOUND reply_in_chat!")
                    logger.info(f"     chat_username: {action.get('chat_username')}")
                    logger.info(f"     reply_text: {action.get('reply_text', '')[:50]}...")

            if has_reply_in_chat:
                logger.info("\n‚úÖ TEST PASSED: LLM generated reply_in_chat action")
                return True
            else:
                logger.warning("\n‚ö†Ô∏è TEST WARNING: LLM did NOT generate reply_in_chat action")
                return False

    except Exception as e:
        logger.error(f"Test failed with error: {e}")
        return False


async def main():
    logger.info("=" * 80)
    logger.info("LLM PROMPT ACTION GENERATION TESTS")
    logger.info("Testing with synthetic data matching real warmup conditions")
    logger.info("=" * 80)

    # Run tests multiple times to account for LLM randomness
    dm_successes = 0
    chat_successes = 0
    runs = 3

    for run in range(runs):
        logger.info(f"\n--- Run {run + 1}/{runs} ---")

        if await test_reply_to_dm_generation():
            dm_successes += 1

        if await test_reply_in_chat_generation():
            chat_successes += 1

    logger.info("\n" + "=" * 80)
    logger.info("SUMMARY")
    logger.info("=" * 80)
    logger.info(f"reply_to_dm generation: {dm_successes}/{runs} successful")
    logger.info(f"reply_in_chat generation: {chat_successes}/{runs} successful")

    if dm_successes > 0 and chat_successes > 0:
        logger.info("\n‚úÖ OVERALL: LLM correctly generates new action types")
    elif dm_successes > 0 or chat_successes > 0:
        logger.info("\n‚ö†Ô∏è OVERALL: Partial success - some action types generated")
    else:
        logger.warning("\n‚ùå OVERALL: LLM did not generate expected action types")
        logger.warning("Consider adjusting prompt emphasis or action descriptions")


if __name__ == "__main__":
    asyncio.run(main())
