# ğŸ—ï¸ Heat Up - Architecture Overview

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           External Client                            â”‚
â”‚                    (HTTP POST /warmup/{session_id})                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FastAPI Server (main.py)                     â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  /warmup/{id}       â”‚              â”‚ /warmup-sync/{id}    â”‚      â”‚
â”‚  â”‚  (async)            â”‚              â”‚ (sync)               â”‚      â”‚
â”‚  â”‚  - Quick response   â”‚              â”‚ - Full report        â”‚      â”‚
â”‚  â”‚  - Background tasks â”‚              â”‚ - Waits completion   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                            â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                         â”‚
                â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Agent (llm_agent.py) â”‚   â”‚ Telegram API Client    â”‚
â”‚                           â”‚   â”‚ (telegram_client.py)   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚                        â”‚
â”‚  â”‚ GPT-4o-mini        â”‚   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ (OpenAI API)       â”‚   â”‚   â”‚  â”‚ join_chat()     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚            â”‚              â”‚   â”‚  â”‚ send_message()  â”‚   â”‚
â”‚            â–¼              â”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚ invoke_raw()    â”‚   â”‚
â”‚  â”‚ Generate Actions   â”‚   â”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ - 3-7 steps       â”‚   â”‚   â”‚  â”‚ get_dialogs()   â”‚   â”‚
â”‚  â”‚ - Diverse channelsâ”‚   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚ - Natural timing  â”‚   â”‚   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚            â”‚              â”‚            â”‚
â”‚            â–¼              â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚            â”‚
â”‚  â”‚ Validate & Sanitizeâ”‚   â”‚            â”‚
â”‚  â”‚ - Check actions    â”‚   â”‚            â”‚
â”‚  â”‚ - Limit durations  â”‚   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
             â”‚                           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Action Executor       â”‚
             â”‚ (executor.py)         â”‚
             â”‚                       â”‚
             â”‚ For each action:      â”‚
             â”‚  1. Execute           â”‚
             â”‚  2. Log result        â”‚
             â”‚  3. Natural delay     â”‚
             â”‚  4. Next action       â”‚
             â”‚                       â”‚
             â”‚ Returns summary       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ External Telegram API â”‚
             â”‚ (from openapi.json)   â”‚
             â”‚                       â”‚
             â”‚ - Session management  â”‚
             â”‚ - RPC methods         â”‚
             â”‚ - Chat operations     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Details

### 1. FastAPI Server (`main.py`)

**Responsibilities:**
- HTTP request handling
- Input validation
- Response formatting
- Background task management
- Lifecycle management

**Endpoints:**
- `POST /warmup/{session_id}` - Async warmup
- `POST /warmup-sync/{session_id}` - Sync warmup
- `GET /health` - Health check
- `GET /` - Service info

**Key Features:**
- Async/await support
- Background tasks via BackgroundTasks
- Automatic OpenAPI documentation
- Structured logging

### 2. LLM Agent (`llm_agent.py`)

**Responsibilities:**
- Generate action plans via GPT-4o-mini
- Validate LLM output
- Provide fallback strategies
- Ensure diversity

**Key Functions:**
```python
generate_action_plan(session_id) -> List[Action]
- Calls OpenAI API
- Parses JSON response
- Validates actions
- Returns plan or fallback

_validate_actions(actions) -> List[Action]
- Check action types
- Validate parameters
- Limit durations

_get_fallback_actions() -> List[Action]
- Safe default plan
- Used when LLM fails
```

**LLM Prompt Strategy:**
- Provides channel pool
- Requests 3-7 actions
- Encourages diversity
- High temperature (1.0) for variety
- Uses GPT-4o-mini for cost-effectiveness

### 3. Telegram API Client (`telegram_client.py`)

**Responsibilities:**
- Wrap Telegram API calls
- Handle authentication
- Error handling
- Request/response parsing

**Key Methods:**
```python
join_chat(session_id, chat_id)
- Join channel/chat
- Returns success/error

send_message(session_id, chat_id, text)
- Send message
- Optional silent mode

invoke_raw(session_id, query)
- Execute arbitrary TL query
- Full flexibility

get_dialogs(session_id)
- Get user's chats
- Used for "browsing" simulation
```

**Features:**
- Async HTTP client (httpx)
- Automatic retries
- Timeout handling
- Detailed error logging

### 4. Action Executor (`executor.py`)

**Responsibilities:**
- Execute action sequences
- Natural timing
- Progress tracking
- Error collection

**Execution Flow:**
```
For each action in plan:
  1. Log start
  2. Execute action
     - join_channel: Call API
     - read_messages: Get dialogs + wait
     - idle: Just wait
  3. Record result
  4. Natural delay (3-10s, random)
  5. Next action

Return execution summary
```

**Natural Delays:**
- Base: 3-10 seconds random
- Occasional longer: +5-10 seconds (10% chance)
- Simulates human behavior

### 5. Configuration (`config.py`)

**Contents:**
```python
Settings:
- API keys
- Base URLs
- Log level

CHANNEL_POOL:
- 20+ channels
- Diverse categories
- Username + description

ACTION_DELAYS:
- Min/max between actions
- Min/max read times
```

## Data Flow

### Typical Request Flow:

```
1. Client Request
   POST /warmup/abc123
   
2. FastAPI Handler
   - Validate session_id
   - Call LLM agent
   
3. LLM Generation
   - Build prompt with channels
   - Call OpenAI API
   - Parse JSON response
   
4. Validation
   - Check action types
   - Validate parameters
   - Apply limits
   
5. Execution (Background or Sync)
   - For each action:
     a. Execute via Telegram client
     b. Wait natural delay
   - Collect results
   
6. Response
   Async: Return plan immediately
   Sync: Return full summary

7. Logging
   - Request received
   - Plan generated
   - Each action executed
   - Final summary
```

### Example Action Plan:

```json
{
  "session_id": "abc123",
  "action_plan": [
    {
      "action": "join_channel",
      "channel_username": "@telegram",
      "reason": "Official updates"
    },
    {
      "action": "read_messages", 
      "channel_username": "@telegram",
      "duration_seconds": 8,
      "reason": "Browse posts"
    },
    {
      "action": "idle",
      "duration_seconds": 5,
      "reason": "Short break"
    }
  ]
}
```

### Execution Timeline:

```
t=0s:    Join @telegram
t=3s:    Wait 3s delay
t=3s:    Start reading @telegram
t=11s:   Finish reading (8s duration)
t=14s:   Wait 3s delay  
t=14s:   Start idle
t=19s:   Finish idle (5s duration)
t=22s:   Wait 3s delay
t=22s:   Complete - return summary
```

## Error Handling

### Graceful Degradation:

```
Level 1: LLM fails
â””â”€> Use fallback action plan

Level 2: Action fails
â””â”€> Log error, continue with next action

Level 3: Telegram API error
â””â”€> Record in results, don't crash

Level 4: Critical error
â””â”€> Return error response, log details
```

### Retry Strategy:

- **LLM calls**: No retry (expensive), use fallback
- **Telegram API**: Configured retries (default: 10)
- **HTTP requests**: httpx default retry logic

## Scalability Considerations

### Current Limitations:
- Single-threaded per request
- No persistent storage
- Synchronous LLM calls

### Scaling Options:

**Horizontal Scaling:**
```bash
# Multiple workers
uvicorn main:app --workers 4

# Load balancer
nginx -> worker1, worker2, worker3, worker4
```

**Async Optimization:**
```python
# Batch LLM calls
plans = await asyncio.gather(*[
    agent.generate_action_plan(sid) 
    for sid in session_ids
])

# Parallel execution
await asyncio.gather(*[
    executor.execute_action_plan(sid, plan)
    for sid, plan in zip(session_ids, plans)
])
```

**Caching:**
- Cache channel list
- Cache LLM responses (with variation)
- Redis for distributed cache

### Production Recommendations:

1. **Add Database:**
   - Store execution history
   - Track success rates
   - Analyze patterns

2. **Add Queue:**
   - Celery/RQ for task queue
   - Handle spikes in requests
   - Retry failed tasks

3. **Add Monitoring:**
   - Prometheus metrics
   - Grafana dashboards
   - Error alerting

4. **Add Rate Limiting:**
   - Per IP
   - Per session
   - API key quotas

## Security Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ HTTPS
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rate Limiter â”‚ â† slowapi/nginx
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚ â† Input validation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI API  â”‚   â”‚ Telegram API â”‚
â”‚ (API key)   â”‚   â”‚ (API key)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Security Measures:**
- âœ… Environment variables for secrets
- âœ… Input validation (pydantic)
- âœ… Timeouts on all requests
- âœ… Error message sanitization
- âš ï¸ No authentication (add if needed)
- âš ï¸ No rate limiting (add if needed)

## Testing Strategy

### Unit Tests:
```python
tests/
â”œâ”€â”€ test_llm_agent.py
â”‚   â”œâ”€â”€ test_generate_plan
â”‚   â”œâ”€â”€ test_validation
â”‚   â””â”€â”€ test_fallback
â”œâ”€â”€ test_executor.py
â”‚   â”œâ”€â”€ test_execute_action
â”‚   â”œâ”€â”€ test_delays
â”‚   â””â”€â”€ test_error_handling
â”œâ”€â”€ test_telegram_client.py
â”‚   â”œâ”€â”€ test_join_chat
â”‚   â””â”€â”€ test_send_message
â””â”€â”€ test_api.py
    â”œâ”€â”€ test_warmup_endpoint
    â””â”€â”€ test_health_endpoint
```

### Integration Tests:
```python
# End-to-end test
async def test_full_warmup():
    response = await client.post("/warmup/test_id")
    assert response.status_code == 200
    assert "action_plan" in response.json()
```

### Load Tests:
```bash
# Using locust or k6
k6 run load_test.js
```

## Monitoring Points

### Key Metrics:
- Request rate (req/s)
- Response time (p50, p95, p99)
- Success rate (%)
- LLM call time (ms)
- Telegram API call time (ms)
- Error rate by type

### Log Levels:
```
DEBUG: Delays, LLM responses
INFO:  Requests, action execution
WARN:  Failed actions, fallbacks
ERROR: API errors, critical failures
```

### Health Checks:
```bash
# Liveness (is it running?)
GET /health

# Readiness (can it serve?)
GET /health
- Check LLM agent initialized
- Check Telegram client ready
```

---

**Architecture Version**: 1.0.0  
**Last Updated**: 2024-10-10  
**Status**: Production-ready MVP

